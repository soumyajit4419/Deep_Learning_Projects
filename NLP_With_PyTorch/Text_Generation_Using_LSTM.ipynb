{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generation_Using_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cudDv02HmZT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXfmWoMZmrHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0cDLUS0pxlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltc1cgFaoxh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/irish-lyrics-eof.txt','r') as f:\n",
        "  text=f.read().splitlines()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mumFuXCYpfxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting to lower and removing the punctuations and whitespaces\n",
        "cleaned_text = []\n",
        "for lines in text:\n",
        "  lines = re.sub('[^a-zA-Z]', ' ',lines)\n",
        "  lines = lines.lower()\n",
        "  lines = lines.strip()\n",
        "  cleaned_text.append(lines)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cCJftOEEW4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary of all words\n",
        "\n",
        "# Creating a list of all words\n",
        "all_words = []\n",
        "for line in cleaned_text:\n",
        "  for word in line.split():\n",
        "    all_words.append(word)\n",
        "  \n",
        "# Removing the duplicate words using set\n",
        "all_words = set(all_words)\n",
        "\n",
        "# Creating a dictionary\n",
        "vocab_dict = {}\n",
        "for i,word in enumerate(all_words):\n",
        "  vocab_dict[word] = i\n",
        "\n",
        "# Adding a special token for unseen words\n",
        "vocab_dict['UNK'] = max(vocab_dict.values()) + 1\n",
        "\n",
        "# Soring the dictionary based on keys\n",
        "vocab_dict = {item:val for item,val in sorted(vocab_dict.items())}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in4QdGcB3Gal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the words in to sequence of numbers\n",
        "def get_sequence(vocab_dict,line):\n",
        "  token2idx = [vocab_dict[word] if word in vocab_dict.keys() else vocab_dict['UNK'] for word in line ]\n",
        "  return token2idx"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPM2OLczD0Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting all the sequence to same length (Padding)\n",
        "def pad_sequence(sequence,max_len=30):\n",
        "  seq = np.zeros(max_len,dtype=int)\n",
        "  len_seq = min(len(sequence),max_len)\n",
        "  seq[-len_seq:] = sequence[:len_seq]\n",
        "  return seq"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK3iNaa5HzXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating ngram tokens of words for inputs\n",
        "n_gram_tokens = []\n",
        "for line in cleaned_text:\n",
        "  line = line.split()\n",
        "  for i in range(1,len(line)):\n",
        "    n_gram_tokens.append(line[:i+1])\n",
        "\n",
        "\n",
        "# Converting tokens to index\n",
        "sequences = []\n",
        "for token in n_gram_tokens:\n",
        "  seq = get_sequence(vocab_dict,token)\n",
        "  sequences.append(seq)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bxHe1dlJ_NH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sequneces\n",
        "padded_sequence = []\n",
        "for sequence in sequences:\n",
        "  pad_seq = pad_sequence(sequence)\n",
        "  padded_sequence.append(pad_seq)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9V58znSGOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating inputs \n",
        "inputs = []\n",
        "for i in padded_sequence:\n",
        "  inputs.append(i[:-1])\n",
        "  \n",
        "# Converting list to array\n",
        "inputs = np.array(inputs)\n",
        "\n",
        "# Converting to torch tensor\n",
        "inputs = torch.LongTensor(inputs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFSHatTMSrm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating target\n",
        "target = []\n",
        "for i in padded_sequence:\n",
        "  target.append(i[-1])\n",
        "  \n",
        "# Converting list to array\n",
        "target = np.array(target)\n",
        "# Reshaping\n",
        "target = target.reshape(-1,1)\n",
        "\n",
        "# Converting to torch tensor\n",
        "target = torch.FloatTensor(target)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwsUFQczTwlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the dataset\n",
        "dataset = TensorDataset(inputs,target)\n",
        "\n",
        "# Creating the dataloader\n",
        "train_loader = DataLoader(dataset,batch_size=128,shuffle=True,num_workers=3,pin_memory=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pNTNDwxWGIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}